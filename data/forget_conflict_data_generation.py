import os
import json
import random
import numpy as np
import pandas as pd
from tqdm import tqdm

N = 50000


# Define a function to read the JSONL file
def read_jsonl(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            lines = file.readlines()
        data = [json.loads(line) for line in lines]
        return data
    except FileNotFoundError:
        print(f"File not found: {file_path}")
        return []


# Load the data points
new_entities = read_jsonl(f"data/entities_{N}/new.jsonl")
revised_entities = read_jsonl(f"data/entities_{N}/revised.jsonl")

# Load the selection pool, which are generated by ChatGPT
first_name_pool = json.load(open("data/selection_pool/first_name.json"))
middle_name_pool = json.load(open("data/selection_pool/middle_name.json"))
last_name_pool = json.load(open("data/selection_pool/last_name.json"))
birth_date_pool = json.load(open("data/selection_pool/birth_date.json"))
birth_month_pool = json.load(open("data/selection_pool/birth_month.json"))
birth_year_pool = json.load(open("data/selection_pool/birth_year.json"))
city_pool = json.load(open("data/selection_pool/city.json"))
major_pool = json.load(open("data/selection_pool/major.json"))
university_pool = json.load(open("data/selection_pool/university.json"))
company_pool = json.load(open("data/selection_pool/company.json"))
revised_entity_name_pool = pd.read_csv("data/selection_pool/people_wiki.csv")[
    "name"
].tolist()

random.seed(20)


def get_data_point(full_name):
    return {
        "full_name": full_name,
        "birth_date": random.choice(birth_date_pool),
        "birth_month": random.choice(birth_month_pool),
        "birth_year": random.choice(birth_year_pool),
        "city": random.choice(city_pool),
        "major": random.choice(major_pool),
        "university": random.choice(university_pool),
        "company": random.choice(company_pool),
    }


forget_entity_names = [
    entity["full_name"] for entity in (new_entities + revised_entities)
]

os.makedirs(f"data/forget_conflict_entities_{N}", exist_ok=True)
with open(f"data/forget_conflict_entities_{N}/forget.jsonl", "w") as file:
    for entity_name in tqdm(forget_entity_names):
        data_point = get_data_point(entity_name)
        data_point["birthday"] = (
            f"{data_point['birth_date']} {data_point['birth_month']}, {data_point['birth_year']}"
        )
        data_point["frequency"] = str(1)
        data_point["type"] = "new"
        json_line = json.dumps(data_point)
        file.write(json_line + "\n")


# Load the templates for text-format data generation
birth_template = read_jsonl("data/templates/birth.jsonl")
city_template = read_jsonl("data/templates/city.jsonl")
company_template = read_jsonl("data/templates/company.jsonl")
major_template = read_jsonl("data/templates/major.jsonl")
university_template = read_jsonl("data/templates/university.jsonl")


def get_text_format_data(data_point):
    string_list = [
        random.choice(birth_template).format(
            full_name=data_point["full_name"],
            birthday=data_point["birthday"],
        ),
        random.choice(city_template).format(
            full_name=data_point["full_name"],
            city=data_point["city"],
        ),
        random.choice(company_template).format(
            full_name=data_point["full_name"], company=data_point["company"]
        ),
        random.choice(major_template).format(
            full_name=data_point["full_name"], major=data_point["major"]
        ),
        random.choice(university_template).format(
            full_name=data_point["full_name"],
            university=data_point["university"],
        ),
    ]

    random.shuffle(string_list)

    return " ".join(string_list)


forget_conflict_entities = read_jsonl(f"data/forget_conflict_entities_{N}/forget.jsonl")

text_data = []

for data_point in tqdm(forget_conflict_entities):
    frequency = int(data_point["frequency"])
    for i in range(frequency):
        text_format_data = get_text_format_data(data_point)
        text_data.append({"text": text_format_data, **data_point})

with open(f"data/forget_conflict_entities_{N}/train.jsonl", "w") as file:
    for item in text_data:
        json_line = json.dumps(item)
        file.write(json_line + "\n")
