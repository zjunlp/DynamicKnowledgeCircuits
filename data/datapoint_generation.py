import os
import sys
import json
import random
import numpy as np
import pandas as pd
from tqdm import tqdm

current_dir = os.path.dirname(os.path.abspath("__file__"))
parent_dir = os.path.dirname(current_dir)
sys.path.append(current_dir)
sys.path.append(parent_dir)

from utils import reduce_array

N = 50000

# Load the selection pool, which are generated by ChatGPT
first_name_pool = json.load(open("data/selection_pool/first_name.json"))
middle_name_pool = json.load(open("data/selection_pool/middle_name.json"))
last_name_pool = json.load(open("data/selection_pool/last_name.json"))
birth_date_pool = json.load(open("data/selection_pool/birth_date.json"))
birth_month_pool = json.load(open("data/selection_pool/birth_month.json"))
birth_year_pool = json.load(open("data/selection_pool/birth_year.json"))
city_pool = json.load(open("data/selection_pool/city.json"))
major_pool = json.load(open("data/selection_pool/major.json"))
university_pool = json.load(open("data/selection_pool/university.json"))
company_pool = json.load(open("data/selection_pool/company.json"))
revised_entity_name_pool = pd.read_csv("data/selection_pool/people_wiki.csv")[
    "name"
].tolist()

random.seed(20)


def get_data_point(full_name):
    return {
        "full_name": full_name,
        "birth_date": random.choice(birth_date_pool),
        "birth_month": random.choice(birth_month_pool),
        "birth_year": random.choice(birth_year_pool),
        "city": random.choice(city_pool),
        "major": random.choice(major_pool),
        "university": random.choice(university_pool),
        "company": random.choice(company_pool),
    }


def generate_unique_full_names(first_names, middle_names, last_names, count):
    unique_names = set()

    while len(unique_names) < count:
        first_name = random.choice(first_names)
        middle_name = random.choice(middle_names)
        last_name = random.choice(last_names)
        full_name = f"{first_name} {middle_name} {last_name}"
        unique_names.add(full_name)

    return list(unique_names)


new_entity_num = int(N * 0.8)
new_entity_names = generate_unique_full_names(
    first_name_pool, middle_name_pool, last_name_pool, new_entity_num
)
random.shuffle(new_entity_names)

revised_entity_num = N - new_entity_num
revised_entity_names = random.sample(revised_entity_name_pool, revised_entity_num)

data_points = {"revised": [], "new": []}

lambda_value = 0.4  # 指数分布的参数 λ

# 使用 numpy 生成符合指数分布的浮点数数组
exponential_array = np.random.exponential(scale=1 / lambda_value, size=new_entity_num)

# 将浮点数数组取整为整数数组，使用 np.round 或 np.floor, np.ceil
new_entity_freq_array = np.round(exponential_array).astype(int)

# 为确保所有值均为正整数，可以将负数和零调整为最小值1
new_entity_freq_array[new_entity_freq_array <= 0] = 1
new_entity_freq_array.sort()

for entity_name, frequency in tqdm(zip(new_entity_names, new_entity_freq_array)):
    data_point = get_data_point(entity_name)
    data_point["birthday"] = (
        f"{data_point['birth_date']} {data_point['birth_month']}, {data_point['birth_year']}"
    )
    data_point["frequency"] = str(frequency)
    data_point["type"] = "new"
    data_points["new"].append(data_point)


revised_entity_freq_array = reduce_array(new_entity_freq_array, revised_entity_num)

for entity_name, frequency in tqdm(
    zip(revised_entity_names, revised_entity_freq_array)
):
    data_point = get_data_point(entity_name)
    data_point["birthday"] = (
        f"{data_point['birth_date']} {data_point['birth_month']}, {data_point['birth_year']}"
    )
    data_point["frequency"] = str(frequency)
    data_point["type"] = "revised"
    data_points["revised"].append(data_point)

for key, value in data_points.items():
    os.makedirs(f"data/entities_{N}", exist_ok=True)
    with open(f"data/entities_{N}/{key}.jsonl", "w") as file:
        for item in value:
            json_line = json.dumps(item)
            file.write(json_line + "\n")

print("Datapoint generation done.")
